{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:16:15.922402Z",
     "start_time": "2024-07-16T12:16:13.595130Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import faiss\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### CLIP for image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:16:19.426229Z",
     "start_time": "2024-07-16T12:16:19.422586Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, processor):\n",
    "        self.image_paths = image_paths\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        return self.processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0)  # 删除多余的维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:16:23.139374Z",
     "start_time": "2024-07-16T12:16:23.135793Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def encode_images(dataset, model):\n",
    "    model.eval()\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                              batch_size=300,\n",
    "                                              pin_memory=True,\n",
    "                                              num_workers=4,\n",
    "                                              prefetch_factor=2,\n",
    "                                              shuffle=False)\n",
    "    all_features = []\n",
    "    with torch.no_grad():  # 关闭梯度计算以提高性能并减少内存使用\n",
    "        for images in tqdm(data_loader):\n",
    "            images = images.to(\"cuda\")  # 假设使用 CUDA\n",
    "            features = model.get_image_features(images)#.pooler_output\n",
    "            all_features.append(features.cpu())  # 将特征移动到 CPU 并保存\n",
    "\n",
    "    return torch.cat(all_features)  # 合并所有特征为一个 Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:16:32.657578Z",
     "start_time": "2024-07-16T12:16:28.238793Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# 初始化 CLIP 处理器和模型\n",
    "clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-large-patch14-336')\n",
    "clip_model = CLIPModel.from_pretrained('openai/clip-vit-large-patch14-336').to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VisDiag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:39:17.425394Z",
     "start_time": "2024-07-16T12:16:34.614709Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [22:42<00:00,  5.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# 加载图片路径\n",
    "\n",
    "search_space_path = json.load(open('./playground/data/css_data/Search_Space_val_50k.json'))\n",
    "merge_image_path = ['./playground/data/css_data/' + p for p in search_space_path]\n",
    "\n",
    "merge_image_path = sorted(merge_image_path)\n",
    "id2image = dict(zip(range(len(merge_image_path)), merge_image_path))\n",
    "with open('./checkpoints/id2image_clip_visdial.pickle', 'wb') as f:\n",
    "    pickle.dump(id2image, f)\n",
    "\n",
    "dataset = ImageDataset(merge_image_path, clip_processor)\n",
    "# 编码图片\n",
    "encoded_images = encode_images(dataset, clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:39:26.400302Z",
     "start_time": "2024-07-16T12:39:25.603520Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "encoded_images = encoded_images.numpy()\n",
    "encoded_images /= np.linalg.norm(encoded_images, axis=1, keepdims=True) \n",
    "\n",
    "# 创建 Faiss 索引\n",
    "dimension = encoded_images.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(encoded_images)\n",
    "faiss.write_index(faiss_index,'./checkpoints/clip_faiss_visdial.index')\n",
    "\n",
    "# save the embedding \n",
    "with open('./checkpoints/clip_image_embedding_visdial.pickle', 'wb') as f:\n",
    "    pickle.dump(encoded_images, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flickr30K数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T07:45:27.726154Z",
     "start_time": "2024-07-13T07:30:55.603530Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [14:25<00:00,  8.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# 相同的代码在不同的数据集上运行\n",
    "# 加载图片路径\n",
    "image_fold = './playground/data/flickr30k/flickr30k_images'\n",
    "image_paths = os.listdir(image_fold)#[:50]\n",
    "merge_image_path = [os.path.join(image_fold, p) for p in image_paths]\n",
    "merge_image_path = sorted(merge_image_path)\n",
    "id2image = dict(zip(range(len(merge_image_path)), merge_image_path))\n",
    "with open('./checkpoints/id2image_clip_flickr30k.pickle', 'wb') as f:\n",
    "    pickle.dump(id2image, f)\n",
    "\n",
    "dataset = ImageDataset(merge_image_path, clip_processor)\n",
    "# 编码图片\n",
    "encoded_images = encode_images(dataset, clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T08:19:02.460193Z",
     "start_time": "2024-07-13T08:19:01.926540Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_images = encoded_images.numpy()\n",
    "encoded_images /= np.linalg.norm(encoded_images, axis=1, keepdims=True) \n",
    "\n",
    "# 创建 Faiss 索引\n",
    "dimension = encoded_images.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(encoded_images)\n",
    "faiss.write_index(faiss_index,'./checkpoints/clip_faiss_flickr30k.index')\n",
    "\n",
    "# save the embedding \n",
    "with open('./checkpoints/clip_image_embedding_flickr30k.pickle', 'wb') as f:\n",
    "    pickle.dump(encoded_images, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSCOCO数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T08:21:28.168407Z",
     "start_time": "2024-07-13T08:19:09.280943Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [02:18<00:00,  5.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# 相同的代码在不同的数据集上运行\n",
    "# 加载图片路径\n",
    "image_fold = './playground/data/mscoco/val2017'\n",
    "image_paths = os.listdir(image_fold)#[:50]\n",
    "merge_image_path = [os.path.join(image_fold, p) for p in image_paths]\n",
    "merge_image_path = sorted(merge_image_path)\n",
    "id2image = dict(zip(range(len(merge_image_path)), merge_image_path))\n",
    "with open('./checkpoints/id2image_clip_mscoco.pickle', 'wb') as f:\n",
    "    pickle.dump(id2image, f)\n",
    "\n",
    "dataset = ImageDataset(merge_image_path, clip_processor)\n",
    "# 编码图片\n",
    "encoded_images = encode_images(dataset, clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T08:21:28.250150Z",
     "start_time": "2024-07-13T08:21:28.169668Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_images = encoded_images.numpy()\n",
    "encoded_images /= np.linalg.norm(encoded_images, axis=1, keepdims=True) \n",
    "\n",
    "# 创建 Faiss 索引\n",
    "dimension = encoded_images.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(encoded_images)\n",
    "faiss.write_index(faiss_index,'./checkpoints/clip_faiss_mscoco.index')\n",
    "\n",
    "# save the embedding \n",
    "with open('./checkpoints/clip_image_embedding_mscoco.pickle', 'wb') as f:\n",
    "    pickle.dump(encoded_images, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLIP for image embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T11:59:27.569677Z",
     "start_time": "2024-07-16T11:59:27.563317Z"
    }
   },
   "outputs": [],
   "source": [
    "### Encode image \n",
    "\n",
    "import io, requests\n",
    "import torch \n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(io.BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "import json\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Dataset class for the corpus images (the 50k potential candidates)\"\"\"\n",
    "    def __init__(self, image_paths, processor):\n",
    "        self.image_paths = image_paths\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image_path = load_image(image_path)\n",
    "        image = self.processor(images=image_path, return_tensors='pt')  # Load and prepare image\n",
    "        return {'id': idx, 'pixel_values': image['pixel_values']}\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle the processing of images.\n",
    "    This function will combine the individual samples into a batch.\n",
    "    \"\"\"\n",
    "    ids = [item['id'] for item in batch]\n",
    "    pixel_values = torch.cat([item['pixel_values'] for item in batch])\n",
    "    \n",
    "    return {'id': ids, 'pixel_values': pixel_values}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def encode_images(dataset, model):\n",
    "    model.eval()\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=250,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=2,\n",
    "                                             pin_memory=True,\n",
    "                                             drop_last=False,\n",
    "                                             prefetch_factor=2,\n",
    "                                             collate_fn=custom_collate_fn\n",
    "                                             )\n",
    "    print(\"Preparing corpus (search space)...\")\n",
    "    corpus_vectors = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            image_embeds = model.vision_model(batch['pixel_values'].to(device))[0]\n",
    "            batch_vectors = F.normalize(model.vision_proj(image_embeds[:, 0, :]), dim=-1)\n",
    "            corpus_vectors.append(batch_vectors.cpu())\n",
    "        corpus_vectors = torch.cat(corpus_vectors)\n",
    "    return corpus_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:00:19.723010Z",
     "start_time": "2024-07-16T12:00:15.516302Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "from transformers import AutoProcessor, BlipForImageTextRetrieval\n",
    "# model_id = \"Salesforce/blip-image-captioning-base\"\n",
    "model_id = \"Salesforce/blip-itm-base-coco\"\n",
    "blip_model = BlipForImageTextRetrieval.from_pretrained(model_id)\n",
    "blip_model.to(device)\n",
    "blip_processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T06:48:35.085176Z",
     "start_time": "2024-07-16T06:48:35.083341Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flickr30K数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T14:26:22.680141Z",
     "start_time": "2024-07-16T14:24:50.747433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9445b18d0e0b4ea0800434cfae2bb337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d0e2923431479ebf967c166686245d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/895M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca89f9ace0b04434879e6d58fd5f41f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fa2eb91ffb4058b445cab0e4b919f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/456 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3b772cee664311a662310e27ecbd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24173da756884c2393c99539557ebd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ee590f929a44e99e88a9b7da5cfac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, BlipForImageTextRetrieval\n",
    "\n",
    "blip_model = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-base-flickr\")\n",
    "blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-itm-base-flickr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T11:45:41.234754Z",
     "start_time": "2024-07-16T11:40:56.442381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing corpus (search space)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [04:44<00:00,  2.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# 相同的代码在不同的数据集上运行\n",
    "# 加载图片路径\n",
    "image_fold = './playground/data/flickr30k/flickr30k_images'\n",
    "image_paths = os.listdir(image_fold)#[:50]\n",
    "merge_image_path = [os.path.join(image_fold, p) for p in image_paths]\n",
    "merge_image_path = sorted(merge_image_path)\n",
    "id2image = dict(zip(range(len(merge_image_path)), merge_image_path))\n",
    "with open('./checkpoints/id2image_blip_flickr30k.pickle', 'wb') as f:\n",
    "    pickle.dump(id2image, f)\n",
    "\n",
    "dataset = ImageDataset(merge_image_path, blip_processor)\n",
    "# 编码图片\n",
    "encoded_images = encode_images(dataset, blip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T11:50:06.400879Z",
     "start_time": "2024-07-16T11:50:06.380621Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.__getitem__(0)['pixel_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T11:50:11.139475Z",
     "start_time": "2024-07-16T11:50:11.114472Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoded_images \u001b[38;5;241m=\u001b[39m \u001b[43mencoded_images\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m()\n\u001b[1;32m      2\u001b[0m encoded_images \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(encoded_images, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 创建 Faiss 索引\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "encoded_images = encoded_images.numpy()\n",
    "encoded_images /= np.linalg.norm(encoded_images, axis=1, keepdims=True) \n",
    "\n",
    "# 创建 Faiss 索引\n",
    "dimension = encoded_images.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(encoded_images)\n",
    "faiss.write_index(faiss_index,'./checkpoints/blip_faiss_flickr30k.index')\n",
    "\n",
    "# save the embedding \n",
    "with open('./checkpoints/blip_image_embedding_flickr30k.pickle', 'wb') as f:\n",
    "    pickle.dump(encoded_images, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSCOCO数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T11:51:21.256961Z",
     "start_time": "2024-07-16T11:50:33.102455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing corpus (search space)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:48<00:00,  2.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# 相同的代码在不同的数据集上运行\n",
    "# 加载图片路径\n",
    "image_fold = './playground/data/mscoco/val2017'\n",
    "image_paths = os.listdir(image_fold)#[:50]\n",
    "merge_image_path = [os.path.join(image_fold, p) for p in image_paths]\n",
    "merge_image_path = sorted(merge_image_path)\n",
    "id2image = dict(zip(range(len(merge_image_path)), merge_image_path))\n",
    "with open('./checkpoints/id2image_blip_mscoco.pickle', 'wb') as f:\n",
    "    pickle.dump(id2image, f)\n",
    "\n",
    "dataset = ImageDataset(merge_image_path, blip_processor)\n",
    "# 编码图片\n",
    "encoded_images = encode_images(dataset, blip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T11:51:21.281026Z",
     "start_time": "2024-07-16T11:51:21.258717Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_images = encoded_images.numpy()\n",
    "encoded_images /= np.linalg.norm(encoded_images, axis=1, keepdims=True) \n",
    "\n",
    "# 创建 Faiss 索引\n",
    "dimension = encoded_images.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(encoded_images)\n",
    "faiss.write_index(faiss_index,'./checkpoints/blip_faiss_mscoco.index')\n",
    "\n",
    "# save the embedding \n",
    "with open('./checkpoints/blip_image_embedding_mscoco.pickle', 'wb') as f:\n",
    "    pickle.dump(encoded_images, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visdial Dataset (50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:14:59.275956Z",
     "start_time": "2024-07-16T12:07:30.472147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing corpus (search space)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:28<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "search_space_path = json.load(open('./playground/data/css_data/Search_Space_val_50k.json'))\n",
    "merge_image_path = ['./playground/data/css_data/' + p for p in search_space_path]\n",
    "\n",
    "merge_image_path = sorted(merge_image_path)\n",
    "id2image = dict(zip(range(len(merge_image_path)), merge_image_path))\n",
    "with open('./checkpoints/id2image_blip_visdial.pickle', 'wb') as f:\n",
    "    pickle.dump(id2image, f)\n",
    "\n",
    "dataset = ImageDataset(merge_image_path, blip_processor)\n",
    "# 编码图片\n",
    "encoded_images = encode_images(dataset, blip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:15:03.219521Z",
     "start_time": "2024-07-16T12:15:02.646726Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_images = encoded_images.numpy()\n",
    "encoded_images /= np.linalg.norm(encoded_images, axis=1, keepdims=True) \n",
    "\n",
    "# 创建 Faiss 索引\n",
    "dimension = encoded_images.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(encoded_images)\n",
    "faiss.write_index(faiss_index,'./checkpoints/blip_faiss_visdial.index')\n",
    "\n",
    "# save the embedding \n",
    "with open('./checkpoints/blip_image_embedding_visdial.pickle', 'wb') as f:\n",
    "    pickle.dump(encoded_images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing corpus (search space)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [04:32<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# 相同的代码在不同的数据集上运行\n",
    "# 加载图片路径\n",
    "image_fold = './playground/data/flickr30k/flickr30k_images'\n",
    "image_paths = os.listdir(image_fold)#[:50]\n",
    "merge_image_path = [os.path.join(image_fold, p) for p in image_paths]\n",
    "merge_image_path = sorted(merge_image_path)\n",
    "id2image = dict(zip(range(len(merge_image_path)), merge_image_path))\n",
    "with open('./checkpoints/id2image_blip_flickr30k.pickle', 'wb') as f:\n",
    "    pickle.dump(id2image, f)\n",
    "\n",
    "dataset = ImageDataset(merge_image_path, blip_processor)\n",
    "# 编码图片\n",
    "encoded_images = encode_images(dataset, blip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_images = encoded_images.numpy()\n",
    "encoded_images /= np.linalg.norm(encoded_images, axis=1, keepdims=True) \n",
    "\n",
    "# 创建 Faiss 索引\n",
    "dimension = encoded_images.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(encoded_images)\n",
    "faiss.write_index(faiss_index,'./checkpoints/blip_faiss_flickr30k.index')\n",
    "\n",
    "# save the embedding \n",
    "with open('./checkpoints/blip_image_embedding_flickr30k.pickle', 'wb') as f:\n",
    "    pickle.dump(encoded_images, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-16T11:40:12.692128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP_ITM(\n",
      "  (visual_encoder): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (text_encoder): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (vision_proj): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (text_proj): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (itm_head): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "#图片搜索\n",
    "class ImageEmbedder:\n",
    "    def __init__(self, model, preprocessor):\n",
    "        \"\"\" model projects image to vector, processor load and prepare image to the model\"\"\"\n",
    "        self.model = model\n",
    "        self.processor = preprocessor\n",
    "\n",
    "def BLIP_BASELINE():\n",
    "    from torchvision import transforms\n",
    "    from torchvision.transforms.functional import InterpolationMode\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    import sys\n",
    "    sys.path.insert(0, './BLIP')\n",
    "    from BLIP.models.blip_itm import blip_itm\n",
    "    # load model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_url = './BLIP/model_base_retrieval_coco.pth'\n",
    "    model = blip_itm(pretrained=model_url, image_size=384, vit='base')\n",
    "    # print(model)\n",
    "    \n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # define Image Embedder (raw_image --> img_feature)\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((384, 384), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "\n",
    "    def blip_project_img(image):\n",
    "        embeds = model.visual_encoder(image)\n",
    "        projection = model.vision_proj(embeds[:, 0, :])\n",
    "        return F.normalize(projection, dim=-1)\n",
    "\n",
    "    def blip_prep_image(path):\n",
    "        raw = Image.open(path).convert('RGB')\n",
    "        return transform_test(raw)\n",
    "\n",
    "    image_embedder = ImageEmbedder(blip_project_img, lambda path: blip_prep_image(path))\n",
    "\n",
    "    # define dialog encoder (dialog --> img_feature)\n",
    "    def dialog_encoder(dialog):\n",
    "        text = model.tokenizer(dialog, padding='longest', truncation=True,\n",
    "                               max_length=200,\n",
    "                               return_tensors=\"pt\"\n",
    "                               ).to(device)\n",
    "\n",
    "        text_output = model.text_encoder(text.input_ids, attention_mask=text.attention_mask,\n",
    "                                         return_dict=True, mode='text')\n",
    "\n",
    "        shift = model.text_proj(text_output.last_hidden_state[:, 0, :])\n",
    "        return F.normalize(shift, dim=-1)\n",
    "\n",
    "    return dialog_encoder, image_embedder\n",
    "dialog_encoder, image_embedder = BLIP_BASELINE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T13:49:49.573411Z",
     "start_time": "2024-07-15T13:49:49.569633Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Dataset class for the corpus images (the 50k potential candidates)\"\"\"\n",
    "    def __init__(self, image_paths, preprocessor):\n",
    "        self.image_paths = image_paths\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = self.preprocessor(image_path)  # Load and prepare image\n",
    "        return {'id': idx, 'image': image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T13:50:08.267441Z",
     "start_time": "2024-07-15T13:50:08.262530Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "def encode_images(dataset, image_embedder):\n",
    "    # model.eval()\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=50,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=2,\n",
    "                                             pin_memory=True,\n",
    "                                             drop_last=False,\n",
    "                                             prefetch_factor=2\n",
    "                                             )\n",
    "    print(\"Preparing corpus (search space)...\")\n",
    "    corpus_vectors = []\n",
    "    # corpus_ids = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch_vectors = F.normalize(image_embedder.model(batch['image'].to(device)), dim=-1)\n",
    "            corpus_vectors.append(batch_vectors)\n",
    "            # corpus_ids.append(batch['id'].to(device))\n",
    "\n",
    "        corpus_vectors = torch.cat(corpus_vectors)\n",
    "    return corpus_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T13:59:14.987300Z",
     "start_time": "2024-07-15T13:58:30.560344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing corpus (search space)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:44<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pickle\n",
    "import torch.nn.functional as F \n",
    "from PIL import Image\n",
    "# 加载图片路径\n",
    "image_fold = './playground/data/mscoco/val2017'\n",
    "image_paths = os.listdir(image_fold)#[:20]\n",
    "merge_image_path = [os.path.join(image_fold, p) for p in image_paths]\n",
    "merge_image_path = sorted(merge_image_path)\n",
    "id2image = dict(zip(range(len(merge_image_path)), merge_image_path))\n",
    "with open('./checkpoints/id2image_blip_mscoco.pickle', 'wb') as f:\n",
    "    pickle.dump(id2image, f)\n",
    "\n",
    "dataset = ImageDataset(merge_image_path, image_embedder.processor)\n",
    "# 编码图片\n",
    "encoded_images = encode_images(dataset, image_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T13:59:15.348978Z",
     "start_time": "2024-07-15T13:59:14.988526Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss \n",
    "import pickle\n",
    "encoded_images = encoded_images.cpu().numpy()\n",
    "encoded_images /= np.linalg.norm(encoded_images, axis=1, keepdims=True) \n",
    "\n",
    "# 创建 Faiss 索引\n",
    "dimension = encoded_images.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(encoded_images)\n",
    "faiss.write_index(faiss_index,'./checkpoints/blip_faiss_mscoco.index')\n",
    "\n",
    "# save the embedding \n",
    "with open('./checkpoints/blip_image_embedding_mscoco.pickle', 'wb') as f:\n",
    "    pickle.dump(encoded_images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### CharIR BLIP for image embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T15:02:24.704787Z",
     "start_time": "2024-05-16T15:02:24.697986Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class ImageEmbedder:\n",
    "    def __init__(self, model, preprocessor):\n",
    "        \"\"\" model projects image to vector, processor load and prepare image to the model\"\"\"\n",
    "        self.model = model\n",
    "        self.processor = preprocessor\n",
    "\n",
    "def BLIP_BASELINE():\n",
    "    from torchvision import transforms\n",
    "    from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "    import sys\n",
    "    sys.path.insert(0, './BLIP')\n",
    "    from BLIP.models.blip_itm import blip_itm\n",
    "    # load model\n",
    "    model = blip_itm(pretrained='./BLIP/chatir_weights.ckpt',  # Download from Google Drive, see README.md\n",
    "                     med_config='BLIP/configs/med_config.json',\n",
    "                     image_size=224,\n",
    "                     vit='base')\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # define Image Embedder (raw_image --> img_feature)\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "\n",
    "    def blip_project_img(image):\n",
    "        embeds = model.visual_encoder(image)\n",
    "        projection = model.vision_proj(embeds[:, 0, :])\n",
    "        return F.normalize(projection, dim=-1)\n",
    "\n",
    "    def blip_prep_image(path):\n",
    "        raw = Image.open(path).convert('RGB')\n",
    "        return transform_test(raw)\n",
    "\n",
    "    image_embedder = ImageEmbedder(blip_project_img, lambda path: blip_prep_image(path))\n",
    "\n",
    "    # define dialog encoder (dialog --> img_feature)\n",
    "    def dialog_encoder(dialog):\n",
    "        text = model.tokenizer(dialog, padding='longest', truncation=True,\n",
    "                               max_length=200,\n",
    "                               return_tensors=\"pt\").to(device)\n",
    "\n",
    "        text_output = model.text_encoder(text.input_ids, attention_mask=text.attention_mask,\n",
    "                                         return_dict=True, mode='text')\n",
    "\n",
    "        shift = model.text_proj(text_output.last_hidden_state[:, 0, :])\n",
    "        return F.normalize(shift, dim=-1)\n",
    "\n",
    "    return dialog_encoder, image_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T15:02:41.875193Z",
     "start_time": "2024-05-16T15:02:41.871201Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Dataset class for the corpus images (the 50k potential candidates)\"\"\"\n",
    "    def __init__(self, image_paths, preprocessor):\n",
    "        self.image_paths = image_paths\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = self.preprocessor(image_path)  # Load and prepare image\n",
    "        return {'id': idx, 'image': image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T15:02:42.362262Z",
     "start_time": "2024-05-16T15:02:42.360201Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# dataset = ImageDataset(merge_image_path, image_embedder.processor)\n",
    "#\n",
    "# next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T15:03:19.142017Z",
     "start_time": "2024-05-16T15:03:19.137924Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "def encode_images(dataset, image_embedder):\n",
    "    # model.eval()\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=500,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=2,\n",
    "                                             pin_memory=True,\n",
    "                                             drop_last=False,\n",
    "                                             prefetch_factor=2\n",
    "                                             )\n",
    "    print(\"Preparing corpus (search space)...\")\n",
    "    corpus_vectors = []\n",
    "    # corpus_ids = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch_vectors = F.normalize(image_embedder.model(batch['image'].to(device)), dim=-1)\n",
    "            corpus_vectors.append(batch_vectors)\n",
    "            # corpus_ids.append(batch['id'].to(device))\n",
    "\n",
    "        corpus_vectors = torch.cat(corpus_vectors)\n",
    "    return corpus_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T15:10:02.448149Z",
     "start_time": "2024-05-16T15:03:19.680589Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from ./BLIP/chatir_weights.ckpt\n",
      "Preparing corpus (search space)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [06:37<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# 初始化 CLIP 处理器和模型\n",
    "dialog_encoder, image_embedder = BLIP_BASELINE()\n",
    "# 加载图片路径\n",
    "image_fold = './playground/data/css_data/unlabeled2017'\n",
    "image_paths = os.listdir(image_fold)#[:20]\n",
    "merge_image_path = [os.path.join(image_fold, p) for p in image_paths]\n",
    "merge_image_path = sorted(merge_image_path)\n",
    "id2image = dict(zip(range(len(merge_image_path)), merge_image_path))\n",
    "with open('./checkpoints/id2image.pickle', 'wb') as f:\n",
    "    pickle.dump(id2image, f)\n",
    "\n",
    "dataset = ImageDataset(merge_image_path, image_embedder.processor)\n",
    "# 编码图片\n",
    "encoded_images = encode_images(dataset, image_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T02:00:00.516271Z",
     "start_time": "2024-05-17T02:00:00.447596Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T02:00:01.420002Z",
     "start_time": "2024-05-17T02:00:01.055447Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 创建 Faiss 索引\n",
    "dimension = encoded_images.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(dimension) # \n",
    "faiss_index.add(encoded_images)\n",
    "faiss.write_index(faiss_index,'./checkpoints/blip_faiss.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T02:00:02.796626Z",
     "start_time": "2024-05-17T02:00:02.505220Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('checkpoints/blip_image_embedding.pickle', 'wb') as f:\n",
    "    pickle.dump(encoded_images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T23:45:04.496232Z",
     "start_time": "2024-05-12T23:45:04.492066Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T22:58:54.293939Z",
     "start_time": "2024-05-12T22:58:54.293580Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP for Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T22:58:54.293997Z",
     "start_time": "2024-05-12T22:58:54.293617Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, processor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list of str): List of text strings.\n",
    "            processor (transformers processor): Processor to tokenize the text.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]  # Get the text at the provided index\n",
    "        inputs = self.processor(text=text, return_tensors=\"pt\", padding=True, truncation=True, max_length=100)\n",
    "        return inputs['input_ids'].squeeze(0), inputs['attention_mask'].squeeze(0)\n",
    "\n",
    "def text_data_collector(batch):\n",
    "    input_ids, attention_masks = zip(*batch)\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids,batch_first=True,padding_value=0)\n",
    "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True,padding_value=0)\n",
    "    return input_ids, attention_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T22:58:54.294049Z",
     "start_time": "2024-05-12T22:58:54.293660Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def encode_text(dataset, data_collector, model):\n",
    "    model.eval()\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                              batch_size=50,\n",
    "                                              pin_memory=True,\n",
    "                                              num_workers=4,\n",
    "                                              prefetch_factor=2,\n",
    "                                              shuffle=False,\n",
    "                                              collate_fn=data_collector)\n",
    "    all_features = []\n",
    "    with torch.no_grad():  # 关闭梯度计算以提高性能并减少内存使用\n",
    "        for batch in data_loader:\n",
    "            input_ids, attention_masks = batch\n",
    "            input_ids = input_ids.to(\"cuda\")\n",
    "            attention_masks = attention_masks.to(\"cuda\")\n",
    "            features = model.get_text_features(input_ids=input_ids, attention_mask=attention_masks)#.pooler_output\n",
    "            print(features)\n",
    "            all_features.append(features.cpu())  # 将特征移动到 CPU 并保存\n",
    "    return torch.cat(all_features)  # 合并所有特征为一个 Tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T22:58:57.849624Z",
     "start_time": "2024-05-12T22:58:54.293810Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.7448e-01,  6.2039e-01,  2.1363e-01,  ...,  2.8336e-02,\n",
      "          2.2532e-01,  1.7659e-01],\n",
      "        [ 5.6634e-01,  9.4504e-01,  2.3726e-01,  ..., -1.8392e-01,\n",
      "          2.4562e-01,  1.6248e-01],\n",
      "        [-2.7422e-01, -4.1648e-01,  3.1572e-01,  ..., -7.5832e-02,\n",
      "         -3.1761e-01, -2.7838e-04]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 初始化 CLIP 处理器和模型\n",
    "processor = CLIPProcessor.from_pretrained('openai/clip-vit-large-patch14-336')\n",
    "clip_model = CLIPModel.from_pretrained('openai/clip-vit-large-patch14-336').to(\"cuda\")\n",
    "\n",
    "texts = [\"[PAD]\", \"This is a text dataset example.\", \"We are learning about AI!\"]\n",
    "# Create an instance of the dataset\n",
    "text_dataset = TextDataset(texts, processor)\n",
    "\n",
    "# 编码图片\n",
    "encoded_ = encode_text(text_dataset, text_data_collector, clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T23:19:32.453685Z",
     "start_time": "2024-05-12T23:19:32.410226Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_topk_images(query,\n",
    "                         topk=10,\n",
    "                         faiss_model=None,\n",
    "                         clip_model=None,\n",
    "                         id2image=None,\n",
    "                         processor=None, ):\n",
    "    text_dataset = TextDataset(query, processor)\n",
    "    query_vec = encode_text(text_dataset, text_data_collector, clip_model)\n",
    "    distance, indices = faiss_model.search(np.array([query_vec]), topk)\n",
    "    return [id2image.get(i, None) for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T22:58:57.853811Z",
     "start_time": "2024-05-12T22:58:57.852444Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m query  \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a text dataset example.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe are learning about AI!\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m \u001b[43mretrieve_topk_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfaiss_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mclip_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mid2image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m, in \u001b[0;36mretrieve_topk_images\u001b[0;34m(query, topk, faiss_model, clip_model, id2image, processor)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_topk_images\u001b[39m(query,\n\u001b[1;32m      2\u001b[0m                          topk\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      3\u001b[0m                          faiss_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m                          clip_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m                          id2image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                          processor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ):\n\u001b[0;32m----> 7\u001b[0m     text_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTextDataset\u001b[49m(query, processor)\n\u001b[1;32m      8\u001b[0m     query_vec \u001b[38;5;241m=\u001b[39m encode_text(text_dataset, text_data_collector, clip_model)\n\u001b[1;32m      9\u001b[0m     distance, indices \u001b[38;5;241m=\u001b[39m faiss_model\u001b[38;5;241m.\u001b[39msearch(np\u001b[38;5;241m.\u001b[39marray([query_vec]), topk)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TextDataset' is not defined"
     ]
    }
   ],
   "source": [
    "query  = [\"[PAD]\", \"This is a text dataset example.\", \"We are learning about AI!\"]\n",
    "faiss_model = faiss.read_index('./checkpoints/clip_faiss.index')\n",
    "retrieve_topk_images(query,\n",
    "                     topk=5,\n",
    "                     faiss_model=faiss_model,\n",
    "                     clip_model=None,\n",
    "                     id2image=None,\n",
    "                     processor=None, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
